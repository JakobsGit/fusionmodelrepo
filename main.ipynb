{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMJD7bRulZYouCXZ+eWw90p"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"nROn-DqxLoB0"},"source":["## HYPER-PARAMETER OPTIMIZATION\n","\n","**Content**\n","\n","\n","\n","\n","*   Installing libraries & loading functions and data from repository\n","*   Download and transform stock data into input data\n","*   Load preprocessed input data and create candlestick pattern feature vectors\n","*   Hyper-parameter optimization for neural network models\n","*   Grid-search for random forest model\n","*   Grid-search for ridge/LASSO regression\n","\n","\n","\n","\\\n"]},{"cell_type":"markdown","metadata":{"id":"02S1chE9JCjo"},"source":["**This code block contains**\n","\n","1.   Installation of required libraries\n","2.   Import of helping functions \n","3.   Import of pre-processed utilized data\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"IWAX_CQW8Smp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628886105693,"user_tz":-120,"elapsed":30670,"user":{"displayName":"Jakob Rempel","photoUrl":"","userId":"00566489469099790538"}},"outputId":"92a8c235-c20c-4bfc-d30a-80e8f40484ee"},"source":["!pip install yfinance\n","!pip install h5py scikit-optimize\n","\n","!git clone https://github.com/JakobsGit/MTMLmodels.git repo-dir\n","\n","%cd repo-dir\n","\n","import get_data_functions\n","from get_data_functions import *\n","\n","import preprocessing_data_helpers\n","from preprocessing_data_helpers import *\n","\n","import create_keras_models\n","from create_keras_models import *\n","\n","import create_results\n","from create_results import *\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting yfinance\n","  Downloading yfinance-0.1.63.tar.gz (26 kB)\n","Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.1.5)\n","Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.19.5)\n","Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.23.0)\n","Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.9)\n","Collecting lxml>=4.5.1\n","  Downloading lxml-4.6.3-cp37-cp37m-manylinux2014_x86_64.whl (6.3 MB)\n","\u001b[K     |████████████████████████████████| 6.3 MB 2.9 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24->yfinance) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2.10)\n","Building wheels for collected packages: yfinance\n","  Building wheel for yfinance (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for yfinance: filename=yfinance-0.1.63-py2.py3-none-any.whl size=23918 sha256=3f010be3b977ad4785d81998e28b059d657293c1ff8717110488478fe6957935\n","  Stored in directory: /root/.cache/pip/wheels/fe/87/8b/7ec24486e001d3926537f5f7801f57a74d181be25b11157983\n","Successfully built yfinance\n","Installing collected packages: lxml, yfinance\n","  Attempting uninstall: lxml\n","    Found existing installation: lxml 4.2.6\n","    Uninstalling lxml-4.2.6:\n","      Successfully uninstalled lxml-4.2.6\n","Successfully installed lxml-4.6.3 yfinance-0.1.63\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n","Collecting scikit-optimize\n","  Downloading scikit_optimize-0.8.1-py2.py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 732 kB/s \n","\u001b[?25hRequirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.19.5)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (0.22.2.post1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.0.1)\n","Collecting pyaml>=16.9\n","  Downloading pyaml-21.8.3-py2.py3-none-any.whl (17 kB)\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.4.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n","Installing collected packages: pyaml, scikit-optimize\n","Successfully installed pyaml-21.8.3 scikit-optimize-0.8.1\n","Cloning into 'repo-dir'...\n","remote: Enumerating objects: 107, done.\u001b[K\n","remote: Counting objects: 100% (34/34), done.\u001b[K\n","remote: Compressing objects: 100% (19/19), done.\u001b[K\n","remote: Total 107 (delta 15), reused 34 (delta 15), pack-reused 73\u001b[K\n","Receiving objects: 100% (107/107), 204.62 MiB | 28.01 MiB/s, done.\n","Resolving deltas: 100% (45/45), done.\n","Checking out files: 100% (13/13), done.\n","/content/repo-dir\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uqiMEMDLbjDZ"},"source":["Install ta-lib and define functions using the library"]},{"cell_type":"code","metadata":{"id":"SPuFxII-UNul","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628851816351,"user_tz":-120,"elapsed":39701,"user":{"displayName":"Jakob Rempel","photoUrl":"","userId":"00566489469099790538"}},"outputId":"93deec97-c335-44f8-ad9e-b4af009bbd39"},"source":["!wget https://launchpad.net/~mario-mariomedina/+archive/ubuntu/talib/+files/libta-lib0_0.4.0-oneiric1_amd64.deb -qO libta.deb\n","!wget https://launchpad.net/~mario-mariomedina/+archive/ubuntu/talib/+files/ta-lib0-dev_0.4.0-oneiric1_amd64.deb -qO ta.deb\n","!dpkg -i libta.deb ta.deb\n","!pip install ta-lib\n","import talib\n","\n","def technicalsfeatures(dataset):\n","  candle_names = talib.get_function_groups()['Pattern Recognition']\n","\n","  for candle in candle_names:\n","\n","      dataset[candle] = 0 \n","\n","  for stockindex in np.unique(dataset['Stock']):\n","      \n","      stockindices = dataset.index[dataset['Stock'] == stockindex]\n","\n","      op = dataset.loc[stockindices,'Open']\n","      hi = dataset.loc[stockindices,'High']\n","      lo = dataset.loc[stockindices,'Low']\n","      cl = dataset.loc[stockindices,'Close']\n","\n","      for candle in candle_names:\n","          dataset.loc[stockindices,candle] = getattr(talib, candle)(op, hi, lo, cl)\n","\n","  dataset.iloc[:,12:] = (dataset.iloc[:,12:])/100\n","  return dataset\n","\n","\n","def create_Xlin(y_df, most_frequent_n):\n","    y_df_lin_data = y_df.copy()\n","    y_df_lin_data = technicalsfeatures(y_df_lin_data)\n","\n","    y_df_lin_train = y_df_lin_data[y_df_lin_data.Date <'2015-01-01'] \n","\n","    X_train_lin = y_df_lin_train.copy()\n","    X_train_lin = X_train_lin.iloc[:,18:]\n","\n","    import numpy as np\n","    pattern_freq = np.zeros(X_train_lin.shape[1])\n","    pattern_index = np.zeros(X_train_lin.shape[1])\n","\n","    counter = 0\n","    for pattern in X_train_lin.columns:\n","        pattern_freq[counter] = X_train_lin[X_train_lin[str(pattern)] !=0].shape[0]\n","        counter = counter +1\n","    pattern_freq_ordered = np.sort(pattern_freq)\n","\n","\n","    counter = -1\n","    for pattern in X_train_lin.columns:\n","        counter = counter +1\n","        if X_train_lin[X_train_lin[str(pattern)] !=0].shape[0] >= pattern_freq_ordered[-most_frequent_n]:\n","          pattern_index[counter] = counter\n","          \n","\n","    X_lin = np.zeros((y_df_lin_data.shape[0],pattern_index[pattern_index>0].shape[0]))\n","\n","    mostfreqpatterns = pattern_index[pattern_index>0]\n","\n","    allpatterns = y_df_lin_data.iloc[:,18:]\n","    utilizedpatterns = allpatterns.iloc[:,pattern_index[pattern_index>0]]\n","    X_lin = np.asarray(utilizedpatterns)\n","\n","    return X_lin\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Selecting previously unselected package libta-lib0.\n","(Reading database ... 160837 files and directories currently installed.)\n","Preparing to unpack libta.deb ...\n","Unpacking libta-lib0 (0.4.0-oneiric1) ...\n","Selecting previously unselected package ta-lib0-dev.\n","Preparing to unpack ta.deb ...\n","Unpacking ta-lib0-dev (0.4.0-oneiric1) ...\n","Setting up libta-lib0 (0.4.0-oneiric1) ...\n","Setting up ta-lib0-dev (0.4.0-oneiric1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n","/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n","\n","Collecting ta-lib\n","  Downloading TA-Lib-0.4.21.tar.gz (270 kB)\n","\u001b[K     |████████████████████████████████| 270 kB 7.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ta-lib) (1.19.5)\n","Building wheels for collected packages: ta-lib\n","  Building wheel for ta-lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ta-lib: filename=TA_Lib-0.4.21-cp37-cp37m-linux_x86_64.whl size=1444983 sha256=e27176582a91a5906507cf2c5380ff8c1b03c5bb12075f9f8c727c409b701aaa\n","  Stored in directory: /root/.cache/pip/wheels/32/3c/86/8dfaee7c11df54449f188172fcf66ae6d134f3118d0237df4a\n","Successfully built ta-lib\n","Installing collected packages: ta-lib\n","Successfully installed ta-lib-0.4.21\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FlxFBxX6KJLx"},"source":["**Download and transform stock data into input data**\n","\n","*Note: This takes quite some time. Instead preprocessed data can be imported in the next step*\n","\n","*Note 2: Ta-Lib installation required see previous step*\n"]},{"cell_type":"code","metadata":{"id":"-qCx8e9mU6eC"},"source":["#define input parameters\n","forecastdays = 1\n","approach = 240\n","timesteps = 20\n","n=1\n","returnfeature = 1\n","\n","# get s&p 500 data with highest trade volume from yahoo finance and replace the \"nan\"\n","stockdata = getsp500data(numberofstocks=50,startdate='1999-12-31', enddate='2019-12-31')  \n","replacenans(stockdata)\n","\n","#data preprocessing\n","dataset = createreturncolumn(stockdata,forecastdays,approach)\n","dataset = createtargetcolumn(dataset,approach)\n","dataset = deletedividendentries(dataset)\n","\n","# create feature vectors\n","X, y, y_df  = createseries(dataset, timesteps, n, returnfeature)\n","X = np.reshape(X, (X.shape[0], X.shape[1]*X.shape[2]))\n","\n","# create candlestick pattern feature vectors with the most frequently occurring patterns\n","most_frequent_n = 20\n","X_lin = create_Xlin(y_df, most_frequent_n)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZaNDIUKuKtl6"},"source":["**Load preprocessed input data and create candlestick pattern feature vectors**\n"]},{"cell_type":"code","metadata":{"id":"dT51FsGEAz12"},"source":["dataset = pd.read_csv('dataset_50SP500_stocks.zip')\n","dataset = dataset.drop(columns='Unnamed: 0')\n","\n","y_df = pd.read_csv('df_with_features.zip')\n","y_df = y_df.drop(columns='Unnamed: 0')\n","\n","y = np.asarray(y_df.Target)\n","\n","X_lin = np.load('X_lin.npy')\n","\n","X1 = np.load('X_array_part1.npy')\n","X2 = np.load('X_array_part2.npy')\n","\n","X = np.concatenate((X1,X2), axis = 0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EJtj6oFlK4eV"},"source":["**Hyper-parameter optimization for neural network models**\n","\n","*   LSTM\n","*   GRU\n","*   Fusion ANN\n","*   Fusion LSTM\n","*   Fusion GRU\n"]},{"cell_type":"code","metadata":{"id":"Gc49PK_UA4Z-"},"source":["# -*- coding: utf-8 -*-\n","\n","#import used libraries\n","import numpy as np\n","import math\n","import pandas as pd\n","import os\n","\n","import tensorflow as tf\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM\n","from tensorflow.keras.layers import GRU\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Bidirectional\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.layers import Activation\n","from tensorflow.keras.losses import binary_crossentropy\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","import skopt\n","from skopt import gp_minimize \n","from skopt.space import Real, Categorical, Integer\n","from skopt.utils import use_named_args\n","from skopt import dump, load\n","\n","#set random seeds\n","seed_value= 1\n","\n","os.environ['PYTHONHASHSEED']=str(seed_value)\n","\n","import random\n","random.seed(seed_value)\n","np.random.seed(seed_value)\n","\n","try:\n","    tf.random.set_seed(seed_value)\n","except:\n","    tf.set_random_seed(seed_value)\n","try:\n","  session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n","  sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n","  K.set_session(sess)\n","except:\n","  session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n","  sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n","  tf.compat.v1.keras.backend.set_session(sess)\n","\n","\n","from tensorflow.keras.metrics import *\n","from sklearn.utils import class_weight\n","from sklearn.metrics import roc_auc_score, balanced_accuracy_score, accuracy_score\n","\n","#####################################################\n","# Input / Parameter Definition\n","#####################################################\n","\n","# select approach, timesteps and arch\n","# arch: 'simpleLSTM', 'simpleGRU', 'fusionANN', 'fusionLSTM', 'fusionGRU'\n","\n","# approach 240: classification\n","#----------------\n","# Feature vector: [R_{t-19}, R_{t-19},..., R_{t}], Open, Close, High and Low Prices\n","# where R_t = P_t / P_{t-1} -1 with  R_{t,m}: Return calculated with P_t closing prices \n","# The features are standardized (std dev = 1, mean = 0) with the training data\n","#\n","# Target: if stock return >0 -> 1 else: 0\n","#\n","\n","arch = 'simpleGRU'\n","approach = 240\n","timesteps = 20\n","forecastdays = 1\n","toppercentile=1\n","\n","num_of_iterations = 50\n","\n","# create folder for specific model results\n","try:\n","  os.mkdir(arch)\n","except:\n","  pass\n","\n","#fusion = 1 feeds models with additional X_lin input and creates additional outputs \n","if (arch == 'fusionANN')|(arch=='fusionLSTM')|(arch=='fusionGRU'):\n","  fusion =1\n","else:\n","  fusion =0\n","\n","\n","# define parameter space\n","dim_learning_rate = Real(low=1e-3, high=1e-1, prior='log-uniform', name='learning_rate')\n","dim_num_nodes = Integer(low=20, high=100, name='num_nodes')\n","dim_num_batch_size = Integer(low=7, high=15, name='num_batch') # batch_size= 2^num_batch\n","dim_dropout_rate = Real(low=0.1, high=0.5, prior='log-uniform', name='dropout_rate')\n","\n","\n","dimensions = [dim_learning_rate,\n","              dim_num_nodes,\n","              dim_num_batch_size,\n","              dim_dropout_rate]\n","\n","# define default parameters to start the hyperparameter optimization\n","lr = 0.03\n","nodes = 100\n","batch = 15\n","dropout = 0.1\n","default_parameters = [lr, nodes, batch, dropout]\n","\n","# regulation parameter\n","l2reg = 0.9\n","\n","# initialization of global metric value\n","best_metric = 1.0\n","\n","# calculate foldsize to distribute input data on equally-sized folds\n","number_of_folds = 6\n","last_training_date = '2014-12-31'\n","datevec = np.unique(y_df.Date)\n","last_train_date_index = np.where(datevec==min(y_df.loc[y_df.Date >= last_training_date].Date))[0][0]\n","numberofdays = np.unique(y_df.Date).shape[0]\n","days_for_val_test = numberofdays-last_train_date_index\n","fold_size = int(days_for_val_test/number_of_folds)\n","\n","\n","#define parameter & performance df for all iterations\n","zeroph = np.zeros(1)\n","parameter_dict = {'auc':zeroph, 'lr':zeroph, 'node':zeroph, 'batch':zeroph, 'dropout':zeroph}\n","best_parameter_df = pd.DataFrame(parameter_dict, columns = ['auc','lr','node','batch', 'dropout'], index = range(0,1)) \n","\n","# define dataframe to save the performace on the test set during the hyperparameter optimization \n","test_perf_dict = {'auc':np.zeros(1), 'acc':np.zeros(1),'balacc':np.zeros(1), 'RMSE':np.zeros(1), 'MSE':np.zeros(1),'MAE':np.zeros(1)}\n","test_perf_df = pd.DataFrame(test_perf_dict, columns = ['auc','acc','balacc','RMSE','MSE','MAE']) \n","\n","# run time series validation, returing the average validation loss (out of sample validation)\n","def timeseriesCV(dataset, X,X_lin, y,y_df,fold_size, numberofdays, timesteps, learning_rate, num_nodes, num_batch, dropout_rate, number_of_folds, approach, forecastdays, arch):\n","  \n","  # create empty lists to track metrics\n","  metric_list = []\n","  test_auc = []\n","  test_acc = []\n","  test_balacc = []\n","\n","  val_auc = []\n","  val_acc = []\n","  val_balacc = []\n","\n","  for foldindex in range(1,number_of_folds):\n","\n","    # split input data into train, validation and test sets\n","    X_train, X_val, X_test, X_train_lin, X_val_lin, X_test_lin, y_train_df, y_val_df, y_test_df, y_train, y_val, y_test = splitdata(X,X_lin,y,y_df, fold_size, foldindex, last_train_date_index)\n","    \n","    # standardize data set based on training data\n","    X_train, X_val, X_test = standardize_input(y_train_df, dataset, X_train, X_val, X_test, timesteps, forecastdays)\n","    \n","    # reshape input for LSTM and GRU networks\n","    if (arch == 'fusionLSTM')|(arch == 'fusionGRU')|(arch == 'simpleLSTM')|(arch == 'simpleGRU'):\n","        X_train = np.reshape(X_train, (X_train.shape[0],20,5))\n","        X_val = np.reshape(X_val, (X_val.shape[0],20,5))\n","        X_test = np.reshape(X_test, (X_test.shape[0],20,5))\n","    \n","    # transform target vector into 2d-vector (softmax activation)\n","    y_train_2d = create2dy(y_train)\n","    y_val_2d = create2dy(y_val)\n","    y_test_2d = create2dy(y_test)\n","\n","    # calculate class weights\n","    class_weights = class_weight.compute_class_weight('balanced',\n","                                                np.unique(y_train),\n","                                                y_train)\n","    class_weights = dict(enumerate(class_weights))\n","        \n","    #create model based on input choise\n","    if arch == 'simpleLSTM':\n","        model = create_simple_lstm_model(learning_rate = learning_rate,\n","                          num_nodes= num_nodes,\n","                          dropout_rate = dropout_rate,\n","                          l2reg = l2reg)\n","        \n","    elif arch == 'simpleGRU':\n","        model = create_simple_gru_model(learning_rate = learning_rate,\n","                          num_nodes= num_nodes,\n","                          dropout_rate = dropout_rate,\n","                          l2reg = l2reg)\n","\n","    if arch == 'fusionANN':\n","        model = create_ann_fusion_model(learning_rate = learning_rate,\n","                          num_nodes= num_nodes,\n","                          dropout_rate = dropout_rate,\n","                          X_train_lin = X_train_lin,\n","                          l2reg = l2reg)\n","\n","    elif arch == 'fusionLSTM':\n","        model = create_lstm_fusion_model(learning_rate = learning_rate,\n","                          num_nodes= num_nodes,\n","                          dropout_rate = dropout_rate,\n","                          X_train_lin = X_train_lin,\n","                          l2reg = l2reg) \n","        \n","\n","    elif arch == 'fusionGRU':\n","        model = create_gru_fusion_model(learning_rate = learning_rate,\n","                          num_nodes= num_nodes,\n","                          dropout_rate = dropout_rate,\n","                          X_train_lin = X_train_lin,\n","                          l2reg = l2reg) \n","        \n","    # training stops after a patience period\n","    es = EarlyStopping(monitor='val_binary_crossentropy', \n","                        mode='min', \n","                        verbose=0, \n","                        patience=10)\n","    # model with best validation loss is saved\n","    checkpointer = ModelCheckpoint(filepath=\"modweights.hdf5\",\n","                                    monitor='val_binary_crossentropy',\n","                                    mode ='min',\n","                                    verbose=0, \n","                                    save_best_only=True)\n","\n","    # fit model\n","    if fusion == 1:\n","      history = model.fit(x=[X_train_lin, X_train],\n","                      y=y_train_2d,\n","                      epochs=2,\n","                      batch_size=2**num_batch,\n","                      validation_data=([X_val_lin, X_val],y_val_2d),\n","                      verbose=0,\n","                      callbacks=[es, checkpointer],\n","                      class_weight = class_weights)\n","\n","    else:\n","      history = model.fit(x=X_train,\n","                    y=y_train_2d,\n","                    epochs=2,\n","                    batch_size=2**num_batch,\n","                    validation_data=(X_val,y_val_2d),\n","                    verbose=0,\n","                    callbacks=[es, checkpointer])\n","\n","    # best model is loaded\n","    model.load_weights('modweights.hdf5')   \n","    # best model is saved for each fold\n","    model_dir = str(foldindex) + arch + str(approach) + 'model.h5'\n","    model.save(model_dir)\n"," \n","    # prediction on the train, validation and test sets\n","    if fusion == 1:\n","      y_test_pred = model.predict([X_test_lin, X_test])\n","      y_test_df['Prediction'] = y_test_pred[:,0]\n","      y_test_df['foldindex']=foldindex\n","\n","      y_val_pred = model.predict([X_val_lin, X_val])\n","      y_val_df['Prediction'] = y_val_pred[:,0]\n","      y_val_df['foldindex']=foldindex\n","\n","      y_train_pred = model.predict([X_train_lin, X_train])\n","      y_train_df['Prediction'] = y_train_pred[:,0]\n","      y_train_df['foldindex']=foldindex\n","\n","    if fusion == 0:\n","      y_test_pred = model.predict(X_test)\n","      y_test_df['Prediction'] = y_test_pred[:,0]\n","      y_test_df['foldindex']=foldindex\n","\n","      y_val_pred = model.predict(X_val)\n","      y_val_df['Prediction'] = y_val_pred[:,0]\n","      y_val_df['foldindex']=foldindex\n","\n","      y_train_pred = model.predict(X_train)\n","      y_train_df['Prediction'] = y_train_pred[:,0]\n","      y_train_df['foldindex']=foldindex\n","\n","    if foldindex ==1:\n","      test_df = y_test_df\n","      val_df = y_val_df\n","      train_df = y_train_df\n","    else:\n","      test_df = pd.concat([test_df, y_test_df])\n","      val_df = pd.concat([val_df, y_val_df])\n","      train_df = pd.concat([train_df, y_train_df])\n","\n","    # save the weights of the two additional neurons for interpretability\n","    if arch == 'fusionANN':\n","      weightarray = model.layers[8].get_weights()[0]\n","      if foldindex == 1:\n","        allweights = weightarray\n","      else:\n","        allweights = np.concatenate((allweights, weightarray), axis=1)\n","\n","    elif (arch == 'fusionLSTM')|(arch == 'fusionGRU'):\n","      weightarray = model.layers[4].get_weights()[0]\n","      if foldindex == 1:\n","        allweights = weightarray\n","      else:\n","        allweights = np.concatenate((allweights, weightarray), axis=1)\n","\n","    # add validation loss to metrics list\n","    bce = np.min(history.history['val_binary_crossentropy'])\n","    metric_list.append(bce)\n","\n","    # calculate validation and test metrics\n","    print('val bce: ', bce)\n","    test_auc.append(roc_auc_score(y_test_2d,y_test_pred))\n","    test_acc.append(accuracy_score(y_test,np.round(y_test_pred[:,0])))\n","    test_balacc.append(balanced_accuracy_score(y_test,np.round(y_test_pred[:,0])))\n","\n","    val_auc.append(roc_auc_score(y_val_2d,y_val_pred))\n","    val_acc.append(accuracy_score(y_val,np.round(y_val_pred[:,0])))\n","    val_balacc.append(balanced_accuracy_score(y_val,np.round(y_val_pred[:,0])))\n","\n","  metric_list = [x for x in metric_list if (not math.isnan(x))]\n","  av_metric = np.average(metric_list)\n","  \n","  global best_metric\n","  print(\" \")\n","  print('best acc: ',best_metric )\n","\n","  # save hyperparameter, performance, weights, model for best performing set of hyper parameters \n","  if av_metric < best_metric:\n","    best_metric = av_metric\n","    print('Test AUC: ', np.average(test_auc))\n","    test_perf_df['auc'] = np.average(test_auc)\n","    best_test_accuray =  np.average(test_acc)\n","    print('TEST Accuracy: ', best_test_accuray)\n","    test_perf_df['acc'] = best_test_accuray\n","    best_test_balaccuray = np.average(test_balacc)\n","    print('TEST Balcanced Accuracy: ', best_test_balaccuray)\n","    test_perf_df['balacc'] = best_test_balaccuray\n","    test_perf_dir = arch +'/'+ arch + 'test_performance_' + str(approach)+'_d_'+str(forecastdays) +'.csv'\n","    with open(test_perf_dir, 'w') as csv_file:\n","      test_perf_df.to_csv(path_or_buf=csv_file,  index=False)\n","\n","    print('Val AUC: ', np.average(val_auc))\n","    print('TEST Accuracy: ', np.average(val_acc))\n","    print('TEST Balcanced Accuracy: ', np.average(val_balacc))\n","\n","    best_parameter_df.loc[0,'auc'] = np.average(val_auc)\n","    best_parameter_df.loc[0,'lr'] = learning_rate\n","    best_parameter_df.loc[0,'node'] = num_nodes\n","    best_parameter_df.loc[0,'batch'] = 2**num_batch\n","    best_parameter_df.loc[0,'dropout'] = dropout_rate\n","\n","    best_param_dir = arch +'/'  +arch +  'val_performance_' +str(approach)+'_d_'+str(forecastdays) +'.csv'\n","    with open(best_param_dir, 'w') as csv_file:\n","      best_parameter_df.to_csv(path_or_buf=csv_file,  index=False)\n","\n","    test_df_dir = arch +'/'  + arch + 'test_df_'+  str(approach)+'_d_'+str(forecastdays) +'.csv'\n","    with open(test_df_dir, 'w') as csv_file:\n","      test_df.to_csv(path_or_buf=csv_file,  index=False)\n","    \n","    val_df_dir = arch +'/' + arch +'val_df_' + str(approach)+'_d_'+str(forecastdays) +'.csv'\n","    with open(val_df_dir, 'w') as csv_file:\n","      val_df.to_csv(path_or_buf=csv_file,  index=False)\n","    \n","    train_df_dir = arch +'/'  + arch+'train_df_'  + str(approach)+'_d_'+str(forecastdays) +'.csv'\n","    with open(train_df_dir, 'w') as csv_file:\n","      train_df.to_csv(path_or_buf=csv_file,  index=False)\n","\n","    if fusion ==1:\n","      allweights_dir = arch +'/' + arch +'allweights_' +str(approach)+'_d_'+str(forecastdays) +'.csv'\n","      np.savetxt(allweights_dir, allweights, delimiter=\",\")\n","   \n","    for fold in range(1,number_of_folds):\n","      model_dir = str(fold) + arch + str(approach) + 'model.h5'\n","      bestmodel = tf.keras.models.load_model(model_dir)\n","\n","      model_dir = arch +'/'  +str(fold) + arch + str(approach) + 'bestmodel.h5'\n","      bestmodel.save(model_dir)\n","\n","  return av_metric\n","\n","\n","# optimization function for the hyperparameter optimization\n","@use_named_args(dimensions=dimensions)\n","def fitness(learning_rate,\n","            num_nodes,\n","            num_batch,\n","            dropout_rate):\n","\n","    # Print the hyper-parameters.\n","    print('learning rate: {0:.1e}'.format(learning_rate))\n","    print('num lstm nodes:', num_nodes)\n","    print('batch size:', 2**num_batch)\n","    print('dropout:', dropout_rate)\n","    print()\n","    \n","    # Create the model with a set of hyper-parameters + run time series cross validation     \n","    av_metric = timeseriesCV(dataset, X, X_lin, y, y_df,fold_size, numberofdays, timesteps, learning_rate, num_nodes, num_batch, dropout_rate, number_of_folds, approach, forecastdays, arch) #num_hidden_layers\n","\n","    print()\n","    print(\"Average bce: \", (av_metric))\n","    print()\n","    \n","    return av_metric\n","    \n","\n","##########################################################\n","# HYPERPARAMETER OPTIMIZATION\n","##########################################################\n","\n","\n","# Bayesian optimization\n","search_result = gp_minimize(func=fitness,\n","                            dimensions=dimensions,\n","                            acq_func='EI', # Expected Improvement.\n","                            n_calls=num_of_iterations,\n","                            x0=default_parameters)\n","\n","\n","performance_df, finperformance_df = createresults(arch, approach, forecastdays, toppercentile)\n","\n","print(performance_df)\n","print(finperformance_df)\n","\n","if (arch == 'fusionANN') | (arch == 'fusionLSTM') | (arch == 'fusionGRU'):\n","    bullishmatrix, bearishmatrix = createpredictionmatrices(arch, approach, forecastdays)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2SGHY0tsLVqE"},"source":["**Grid-search for random forest model**"]},{"cell_type":"code","metadata":{"id":"mZjUT_bmBNUb"},"source":["#import used libraries\n","import numpy as np\n","import math\n","import pandas as pd\n","\n","# set random seed\n","seed_value= 1\n","\n","import os\n","os.environ['PYTHONHASHSEED']=str(seed_value)\n","\n","import random\n","random.seed(seed_value)\n","np.random.seed(seed_value)\n","\n","from sklearn.metrics import roc_auc_score, balanced_accuracy_score, accuracy_score\n","from sklearn.metrics import  mean_squared_error, mean_absolute_error, log_loss\n","\n","'''\n","! apt-get install default-jre\n","!java -version\n","! pip install h2o\n","import h2o\n","h2o.init()\n","'''\n","\n","!pip install -q bentoml \"h2o>=3.24.0.2\"\n","\n","import h2o\n","h2o.init(min_mem_size = \"6g\")\n","\n","\n","from h2o.estimators.random_forest import H2ORandomForestEstimator\n","\n","#####################################################\n","# Input / Parameter Definition\n","#####################################################\n","\n","# select approach, timesteps and arch\n","# arch: 'RF'\n","# approach 240: RF approach (classification)\n","#----------------\n","# Feature vector: [R_{t-19}, R_{t-18},..., R_{t}], Open, Close, High and Low Prices\n","# where R_t = P_t / P_{t-1} -1 with  R_{t,m}: Return calculated with P_t closing prices \n","# The features are standardized (std dev = 1, mean = 0) with the training data\n","#\n","# define approach parameters\n","arch = 'RF'\n","approach = 240\n","timesteps = 20\n","forecastdays=1\n","toppercentile=1\n","\n","# create folder for specific model results\n","try:\n","  os.mkdir(arch)\n","except:\n","  pass\n","\n","# initialization of global metric value\n","best_metric = 1.0\n","\n","# calculate foldsize to distribute input data on equally-sized folds\n","number_of_folds = 6\n","last_training_date = '2014-12-31'\n","datevec = np.unique(y_df.Date)\n","last_train_date_index = np.where(datevec==min(y_df.loc[y_df.Date >= last_training_date].Date))[0][0]\n","numberofdays = np.unique(y_df.Date).shape[0]\n","days_for_val_test = numberofdays-last_train_date_index\n","fold_size = int(days_for_val_test/number_of_folds)\n","val_days = fold_size\n","\n","\n","# define dataframe to save the performace on the test set during the hyperparameter optimization \n","test_perf_dict = {'auc':np.zeros(1), 'acc':np.zeros(1),'balacc':np.zeros(1), 'RMSE':np.zeros(1), 'MSE':np.zeros(1),'MAE':np.zeros(1)}\n","test_perf_df = pd.DataFrame(test_perf_dict, columns = ['auc','acc','balacc','RMSE','MSE','MAE']) \n","\n","zeroph = np.zeros(1)\n","parameter_dict = {'auc':zeroph, 'trees':zeroph, 'depth':zeroph}\n","best_parameter_df_RF = pd.DataFrame(parameter_dict, columns = ['auc','trees','depth'], index = range(0,1)) \n","\n","# transform input data intp h2o data frames\n","def createh2oframes(X, y):\n","\n","    indexvec = np.arange(0,len(y))\n","    colnames = np.arange(-X.shape[1],0,1)\n","    \n","    df = pd.DataFrame(data=X[0:,0:],\n","            index=indexvec,    \n","            columns=colnames) \n","    \n","    df['y'] = y\n","    hf = h2o.H2OFrame(df)\n","    if (approach == 31) | (approach == 240):\n","      hf['y'] = hf['y'].asfactor()\n","    \n","    return hf\n","\n","\n","# run time series validation, returing the average validation loss (out of sample validation)\n","def timeseriesCV_RF(dataset,X,X_lin,y,y_df,fold_size, numberofdays, timesteps,ntrees,max_depth, number_of_folds, approach, forecastdays, arch):\n","\n","  #define empty lists to track performance of each fold\n","  metric_list = []\n","  test_auc = []\n","  test_acc = []\n","  test_balacc = []\n","\n","  val_auc = []\n","  val_acc = []\n","  val_balacc = []\n","\n","  for foldindex in range(1,number_of_folds):\n","\n","    #prepare data\n","    X_train, X_val, X_test, X_train_lin, X_val_lin, X_test_lin, y_train_df, y_val_df, y_test_df, y_train, y_val, y_test = splitdata(X,X_lin,y,y_df, fold_size, foldindex, last_train_date_index)  \n","    X_train, X_val, X_test = standardize_input(y_train_df, dataset, X_train, X_val, X_test, timesteps, forecastdays)\n","    \n","    hf_train = createh2oframes(X_train, y_train)\n","    hf_val = createh2oframes(X_val, y_val)\n","    hf_test = createh2oframes(X_test, y_test)\n","\n","    ydata= \"y\"\n","    xdata= hf_train.columns[:-1]\n","\n","    #fit model\n","    rf_fit = H2ORandomForestEstimator(model_id='rf_fit', ntrees=int(ntrees),max_depth=int(max_depth), seed=seed_value, balance_classes=True)\n","    rf_fit.train(x=xdata, y=ydata, training_frame=hf_train, validation_frame = hf_val)\n","\n","    # predict target for training, validation and test sets\n","    y_test_pred = rf_fit.predict(hf_test)\n","    preddf = y_test_pred.as_data_frame(use_pandas=True)\n","    y_test_pred = np.array(preddf['p1'])\n","\n","    y_val_pred = rf_fit.predict(hf_val)\n","    val_preddf = y_val_pred.as_data_frame(use_pandas=True)\n","    y_val_pred = np.array(val_preddf['p1'])\n","\n","    y_train_pred = rf_fit.predict(hf_train)\n","    train_preddf = y_train_pred.as_data_frame(use_pandas=True)\n","    y_train_pred = np.array(train_preddf['p1'])\n","\n","\n","    y_test_df['Prediction'] = y_test_pred\n","    y_test_df['foldindex']=foldindex\n","\n","    y_val_df['Prediction'] = y_val_pred\n","    y_val_df['foldindex']=foldindex\n","\n","    y_train_df['Prediction'] = y_train_pred\n","    y_train_df['foldindex']=foldindex\n","      \n","    \n","    if foldindex ==1:\n","      test_df = y_test_df\n","      val_df = y_val_df\n","      train_df = y_train_df\n","    else:\n","      test_df = pd.concat([test_df, y_test_df])\n","      val_df = pd.concat([val_df, y_val_df])\n","      train_df = pd.concat([train_df, y_train_df])\n","\n","    logloss = log_loss(y_val,y_val_pred)\n","    metric_list.append(logloss)\n","\n","    # calculate metrics for validation and test seta\n","    test_auc.append(roc_auc_score(y_test,y_test_pred))\n","    test_acc.append(accuracy_score(y_test,np.round(y_test_pred)))\n","    test_balacc.append(balanced_accuracy_score(y_test,np.round(y_test_pred)))\n","\n","    val_auc.append(roc_auc_score(y_val,y_val_pred))\n","    val_acc.append(accuracy_score(y_val,np.round(y_val_pred)))\n","    val_balacc.append(balanced_accuracy_score(y_val,np.round(y_val_pred)))\n","\n","    # delete model to create a new model for the next fold\n","    h2o.remove_all()\n","\n","  metric_list = [x for x in metric_list if (not math.isnan(x))]\n","  av_metric = np.average(metric_list)\n","  \n","  global best_metric\n","  print(\" \")\n","  print('best acc: ',best_metric )\n","\n","  # save hyper parameter, performance and prediction for best hyper parameters \n","  if av_metric < best_metric:\n","    best_metric = av_metric\n","    print('Test AUC: ', np.average(test_auc))\n","    test_perf_df['auc'] = np.average(test_auc)\n","    best_test_accuray =  np.average(test_acc)\n","    print('TEST Accuracy: ', best_test_accuray)\n","    test_perf_df['acc'] = best_test_accuray\n","    best_test_balaccuray = np.average(test_balacc)\n","    print('TEST Balcanced Accuracy: ', best_test_balaccuray)\n","    test_perf_df['balacc'] = best_test_balaccuray\n","    test_perf_dir = arch +'/'+ arch + 'test_performance_' + str(approach)+'_d_'+str(forecastdays) +'.csv'\n","    with open(test_perf_dir, 'w') as csv_file:\n","      test_perf_df.to_csv(path_or_buf=csv_file,  index=False)\n","\n","    print('Val AUC: ', np.average(val_auc))\n","    print('Val Accuracy: ', np.average(val_acc))\n","    print('Val Balcanced Accuracy: ', np.average(val_balacc))\n","\n","    best_parameter_df_RF.loc[0,'auc'] = np.average(av_metric)\n","    best_parameter_df_RF.loc[0,'trees'] = ntrees\n","    best_parameter_df_RF.loc[0,'depth'] = max_depth\n","\n","    best_param_dir = arch +'/'  +arch +  'val_performance_' +str(approach)+'_d_'+str(forecastdays) +'.csv'\n","    with open(best_param_dir, 'w') as csv_file:\n","      best_parameter_df_RF.to_csv(path_or_buf=csv_file,  index=False)\n","\n","    test_df_dir = arch +'/'  + arch + 'test_df_'+  str(approach)+'_d_'+str(forecastdays) +'.csv'\n","    with open(test_df_dir, 'w') as csv_file:\n","      test_df.to_csv(path_or_buf=csv_file,  index=False)\n","    \n","    val_df_dir = arch +'/' + arch +'val_df_' + str(approach)+'_d_'+str(forecastdays) +'.csv'\n","    with open(val_df_dir, 'w') as csv_file:\n","      val_df.to_csv(path_or_buf=csv_file,  index=False)\n","    \n","    train_df_dir = arch +'/'  + arch+'train_df_'  + str(approach)+'_d_'+str(forecastdays) +'.csv'\n","    with open(train_df_dir, 'w') as csv_file:\n","      train_df.to_csv(path_or_buf=csv_file,  index=False)\n","\n","  return av_metric\n","\n","\n","##########################################################\n","# HYPERPARAMETER OPTIMIZATION\n","##########################################################\n","\n","# grid search\n","av_metric_list = []\n","for numtrees in [100,300,500,1000,2000]:\n","  for treedepth in [20,10,5]:\n","    av_metric = timeseriesCV_RF(dataset,X,X_lin,y,y_df,fold_size, numberofdays, timesteps,numtrees,treedepth, number_of_folds, approach, forecastdays, arch) #num_hidden_layers\n","    av_metric_list.append(av_metric)\n","    print('trees: ', numtrees, 'depth: ', treedepth)\n","    print(av_metric_list)\n","\n","performance_df, finperformance_df = createresults(arch, approach, forecastdays, toppercentile)\n","\n","print(performance_df)\n","print(finperformance_df)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"00WuLIkULejc"},"source":["**Grid-search for ridge/LASSO regression**"]},{"cell_type":"code","metadata":{"id":"dQxZcMNNBQhI"},"source":["# -*- coding: utf-8 -*-\n","\n","#import used libraries\n","import numpy as np\n","#import math\n","import pandas as pd\n","\n","#set random seed\n","seed_value= 1\n","import os\n","os.environ['PYTHONHASHSEED']=str(seed_value)\n","import random\n","random.seed(seed_value)\n","np.random.seed(seed_value)\n","\n","from sklearn.metrics import roc_auc_score, balanced_accuracy_score, accuracy_score, log_loss\n","\n","'''\n","! apt-get install default-jre\n","!java -version\n","! pip install h2o\n","'''\n","!pip install -q bentoml \"h2o>=3.24.0.2\"\n","\n","import h2o\n","h2o.init(min_mem_size = \"6g\")\n","\n","from h2o.estimators.random_forest import H2ORandomForestEstimator\n","from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n","#####################################################\n","# Input / Parameter Definition\n","#####################################################\n","\n","# select approach, timesteps and arch\n","# arch: 'ridge' or 'lasso'\n","# approach 240: classification\n","#----------------\n","# Feature vector: [R_{t-19}, R_{t-18},..., R_{t}], Open, Close, High and Low Prices\n","# where R_t = P_t / P_{t-1} -1 with  R_{t,m}: Return calculated with P_t closing prices \n","# The features are standardized (std dev = 1, mean = 0) with the training data\n","#\n","\n","fusion = 0\n","arch = 'ridge'\n","approach = 240\n","timesteps = 20\n","forecastdays=1\n","toppercentile=1\n","\n","# create folder for specific model results\n","try:\n","  os.mkdir(arch)\n","except:\n","  pass\n","\n","# initialization of global metric value\n","best_metric = 1.0\n","\n","# calculate foldsize to distribute input data on equally-sized folds\n","number_of_folds = 6\n","last_training_date = '2014-12-31'\n","datevec = np.unique(y_df.Date)\n","last_train_date_index = np.where(datevec==min(y_df.loc[y_df.Date >= last_training_date].Date))[0][0]\n","numberofdays = np.unique(y_df.Date).shape[0]\n","days_for_val_test = numberofdays-last_train_date_index#-val_days\n","fold_size = int(days_for_val_test/number_of_folds)\n","val_days = fold_size\n","\n","\n","# define dataframe to save the performace on the test set during the hyperparameter optimization \n","test_perf_dict = {'auc':np.zeros(1), 'acc':np.zeros(1),'balacc':np.zeros(1), 'RMSE':np.zeros(1), 'MSE':np.zeros(1),'MAE':np.zeros(1)}\n","test_perf_df = pd.DataFrame(test_perf_dict, columns = ['auc','acc','balacc','RMSE','MSE','MAE']) \n","\n","zeroph = np.zeros(1)\n","parameter_dict = {'auc':zeroph, 'lambda_val':zeroph}\n","best_parameter_df_reg = pd.DataFrame(parameter_dict, columns = ['auc','lambda_val'], index = range(0,1)) \n","\n","\n","def createh2oframes(X, y):\n","\n","    indexvec = np.arange(0,len(y))\n","    colnames = np.arange(-X.shape[1],0,1)\n","    \n","    df = pd.DataFrame(data=X[0:,0:],\n","            index=indexvec,    \n","            columns=colnames) \n","      \n","    df['y'] = y\n","    hf = h2o.H2OFrame(df)\n","    if (approach == 31) | (approach == 240):\n","      hf['y'] = hf['y'].asfactor()\n","    \n","    return hf\n","\n","\n","# run time series validation, returing the average validation loss (out of sample validation)\n","def timeseriesCV_reg(dataset,X,X_lin,y,y_df,fold_size, numberofdays, timesteps, lambda_val, number_of_folds, approach, forecastdays, arch):\n","\n","  metric_list = []\n","  test_auc = []\n","  test_acc = []\n","  test_balacc = []\n","\n","  val_auc = []\n","  val_acc = []\n","  val_balacc = []\n","\n","\n","  for foldindex in range(1,number_of_folds):\n","\n","    #prepare data\n","    X_train, X_val, X_test, X_train_lin, X_val_lin, X_test_lin, y_train_df, y_val_df, y_test_df, y_train, y_val, y_test = splitdata(X,X_lin,y,y_df, fold_size, foldindex, last_train_date_index)  \n","    X_train, X_val, X_test = standardize_input(y_train_df, dataset, X_train, X_val, X_test, timesteps, forecastdays)\n","\n","    if arch == 'lasso':\n","      alpha_value = 1\n","    if arch == 'ridge':\n","      alpha_value = 0\n","    \n","    hf_train = createh2oframes(X_train, y_train)\n","    hf_val = createh2oframes(X_val, y_val)\n","    hf_test = createh2oframes(X_test, y_test)\n","\n","    ydata= \"y\"\n","    xdata= hf_train.columns[:-1]\n","\n","    # create + fit model\n","    glm = H2OGeneralizedLinearEstimator(alpha = alpha_value,\n","                                        lambda_ = lambda_val,\n","                                        seed = seed_value)\n","    glm.train(x=xdata, y=ydata, training_frame=hf_train, validation_frame=hf_val)\n","    \n","    # predict target for training, validation and test sets\n","    y_test_pred = glm.predict(hf_test)\n","    preddf = y_test_pred.as_data_frame(use_pandas=True)\n","    y_test_pred = np.array(preddf['p1'])\n","\n","    y_val_pred = glm.predict(hf_val)\n","    val_preddf = y_val_pred.as_data_frame(use_pandas=True)\n","    y_val_pred = np.array(val_preddf['p1'])\n","\n","    y_train_pred = glm.predict(hf_train)\n","    train_preddf = y_train_pred.as_data_frame(use_pandas=True)\n","    y_train_pred = np.array(train_preddf['p1'])\n","\n","    y_test_df['Prediction'] = y_test_pred\n","    y_test_df['foldindex']=foldindex\n","\n","    y_val_df['Prediction'] = y_val_pred\n","    y_val_df['foldindex']=foldindex\n","\n","    y_train_df['Prediction'] = y_train_pred\n","    y_train_df['foldindex']=foldindex\n","         \n","    if foldindex ==1:\n","      test_df = y_test_df\n","      val_df = y_val_df\n","      train_df = y_train_df\n","    else:\n","      test_df = pd.concat([test_df, y_test_df])\n","      val_df = pd.concat([val_df, y_val_df])\n","      train_df = pd.concat([train_df, y_train_df])\n","\n","    logloss = log_loss(y_val,y_val_pred)\n","    metric_list.append(logloss)\n","    # calculate metrics for validation and test seta\n","    test_auc.append(roc_auc_score(y_test,y_test_pred))\n","    test_acc.append(accuracy_score(y_test,np.round(y_test_pred)))\n","    test_balacc.append(balanced_accuracy_score(y_test,np.round(y_test_pred)))\n","\n","    val_auc.append(roc_auc_score(y_val,y_val_pred))\n","    val_acc.append(accuracy_score(y_val,np.round(y_val_pred)))\n","    val_balacc.append(balanced_accuracy_score(y_val,np.round(y_val_pred)))\n","    \n","    # delete model to create a new model for the next fold\n","    h2o.remove_all()\n","\n","  metric_list = [x for x in metric_list if (not math.isnan(x))]\n","  av_metric = np.average(metric_list)\n","  \n","  global best_metric\n","  print(\" \")\n","  print('best acc: ',best_metric )\n","\n","  # save hyper parameter, performance and prediction for best hyper parameters\n","  if av_metric < best_metric:\n","    best_metric = av_metric\n","    print('Test AUC: ', np.average(test_auc))\n","    test_perf_df['auc'] = np.average(test_auc)\n","    best_test_accuray =  np.average(test_acc)\n","    print('TEST Accuracy: ', best_test_accuray)\n","    test_perf_df['acc'] = best_test_accuray\n","    best_test_balaccuray = np.average(test_balacc)\n","    print('TEST Balcanced Accuracy: ', best_test_balaccuray)\n","    test_perf_df['balacc'] = best_test_balaccuray\n","    test_perf_dir = arch +'/'+ arch + 'test_performance_' + str(approach)+'_d_'+str(forecastdays) +'.csv'\n","    with open(test_perf_dir, 'w') as csv_file:\n","      test_perf_df.to_csv(path_or_buf=csv_file,  index=False)\n","\n","    print('Val AUC: ', np.average(val_auc))\n","    print('Val Accuracy: ', np.average(val_acc))\n","    print('Val Balcanced Accuracy: ', np.average(val_balacc))\n","\n","    best_parameter_df_reg.loc[0,'auc'] = np.average(av_metric)\n","    best_parameter_df_reg.loc[0,'lambda_val'] = lambda_val\n","\n","    best_param_dir = arch +'/'  +arch +  'val_performance_' +str(approach)+'_d_'+str(forecastdays) +'.csv'\n","    with open(best_param_dir, 'w') as csv_file:\n","      best_parameter_df_reg.to_csv(path_or_buf=csv_file,  index=False)\n","\n","    test_df_dir = arch +'/'  + arch + 'test_df_'+  str(approach)+'_d_'+str(forecastdays) +'.csv'\n","    with open(test_df_dir, 'w') as csv_file:\n","      test_df.to_csv(path_or_buf=csv_file,  index=False)\n","    \n","    val_df_dir = arch +'/' + arch +'val_df_' + str(approach)+'_d_'+str(forecastdays) +'.csv'\n","    with open(val_df_dir, 'w') as csv_file:\n","      val_df.to_csv(path_or_buf=csv_file,  index=False)\n","    \n","    train_df_dir = arch +'/'  + arch+'train_df_'  + str(approach)+'_d_'+str(forecastdays) +'.csv'\n","    with open(train_df_dir, 'w') as csv_file:\n","      train_df.to_csv(path_or_buf=csv_file,  index=False)\n","\n","  return av_metric\n","\n","\n","##########################################################\n","# HYPERPARAMETER OPTIMIZATION\n","##########################################################\n","\n","# Grid search\n","av_metric_list = []\n","for lambdaval in [0.0000001,0.000001,0.00001,0.0001,0.001,0.01,0.1,0.9]:\n","\n","    av_metric = timeseriesCV_reg(dataset,X,X_lin,y,y_df,fold_size, numberofdays, timesteps, lambdaval, number_of_folds, approach, forecastdays, arch)\n","    av_metric_list.append(av_metric)\n","\n","performance_df, finperformance_df = createresults(arch, approach, forecastdays, toppercentile)\n","\n","print(performance_df)\n","print(finperformance_df)\n"],"execution_count":null,"outputs":[]}]}